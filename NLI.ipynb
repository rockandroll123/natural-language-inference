{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(56) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "62\n",
      "59\n",
      "55\n",
      "57\n",
      "30\n",
      "RNN / Embed / Sent = None, 300, 300\n",
      "GloVe / Trainable Word Embeddings = True, False\n",
      "Build model...\n",
      "Vocab size = 42391\n",
      "Loading GloVe\n",
      "Total number of null word embeddings:\n",
      "4043\n"
     ]
    }
   ],
   "source": [
    "def extract_tokens_from_binary_parse(parse):\n",
    "    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n",
    "\n",
    "def yield_examples(fn, skip_no_majority=True, limit=None):\n",
    "    for i, line in enumerate(open(fn)):\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        label = data['gold_label']\n",
    "        s1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n",
    "        s2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n",
    "        if skip_no_majority and label == '-':\n",
    "            continue\n",
    "        yield (label, s1, s2)\n",
    "\n",
    "def get_data(fn, limit=None):\n",
    "    raw_data = list(yield_examples(fn=fn, limit=limit))\n",
    "    left = [s1 for _, s1, s2 in raw_data]\n",
    "    right = [s2 for _, s1, s2 in raw_data]\n",
    "    print(max(len(x.split()) for x in left))\n",
    "    print(max(len(x.split()) for x in right))\n",
    "\n",
    "    LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "    Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n",
    "    Y = np_utils.to_categorical(Y, len(LABELS))\n",
    "\n",
    "    return left, right, Y\n",
    "\n",
    "def predict_inference(model,text,hypothese):\n",
    "    prepare_data_pred = lambda data: (to_seq(data[0]), to_seq(data[1]))\n",
    "    coded=prepare_data_pred([[text],[hypothese]])\n",
    "    results= model.predict([coded[0],coded[1]])\n",
    "\n",
    "    inference=LABELS_INV[argmax(results)]\n",
    "    print(\"Precicted Judgments: \",inference)\n",
    "    return inference\n",
    "\n",
    "training_org = get_data('snli_1.0_train.jsonl')\n",
    "validation_org = get_data('snli_1.0_dev.jsonl')\n",
    "test_org = get_data('snli_1.0_test.jsonl')\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(training_org[0] + training_org[1])\n",
    "\n",
    "VOCAB = len(tokenizer.word_counts) + 1\n",
    "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "LABELS_INV = dict((v,k) for k,v in LABELS.items())\n",
    "RNN = None\n",
    "LAYERS = 1\n",
    "USE_GLOVE = True\n",
    "TRAIN_EMBED = False\n",
    "EMBED_HIDDEN_SIZE = 300\n",
    "SENT_HIDDEN_SIZE = 300\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 4 # 8\n",
    "MAX_EPOCHS = 10\n",
    "MAX_LEN = 42\n",
    "DP = 0.2\n",
    "L2 = 4e-6\n",
    "ACTIVATION = 'relu'\n",
    "OPTIMIZER = 'rmsprop'\n",
    "print('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\n",
    "print('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))\n",
    "\n",
    "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
    "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
    "prepare_data_pred = lambda data: (to_seq(data[0]), to_seq(data[1]))\n",
    "\n",
    "training = prepare_data(training_org)\n",
    "validation = prepare_data(validation_org)\n",
    "test = prepare_data(test_org)\n",
    "\n",
    "print('Build model...')\n",
    "print('Vocab size =', VOCAB)\n",
    "\n",
    "GLOVE_STORE = 'precomputed_glove.weights'\n",
    "if USE_GLOVE:\n",
    "    if not os.path.exists(GLOVE_STORE + '.npy'):\n",
    "        print('Computing GloVe')\n",
    "        embeddings_index = {}\n",
    "        f = open('glove.840B.300d.txt')\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                print('Missing from GloVe: {}'.format(word))\n",
    "\n",
    "        np.save(GLOVE_STORE, embedding_matrix)\n",
    "\n",
    "    print('Loading GloVe')\n",
    "    embedding_matrix = np.load(GLOVE_STORE + '.npy')\n",
    "\n",
    "    print('Total number of null word embeddings:')\n",
    "    print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "    embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
    "else:\n",
    "    embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n",
    "\n",
    "rnn_kwargs = dict(output_dim=SENT_HIDDEN_SIZE, dropout_W=DP, dropout_U=DP)\n",
    "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
    "\n",
    "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
    "\n",
    "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "\n",
    "prem = embed(premise)\n",
    "hypo = embed(hypothesis)\n",
    "\n",
    "prem = translate(prem)\n",
    "hypo = translate(hypo)\n",
    "\n",
    "if RNN and LAYERS > 1:\n",
    "    for l in range(LAYERS - 1):\n",
    "        rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
    "        prem = BatchNormalization()(rnn(prem))\n",
    "        hypo = BatchNormalization()(rnn(hypo))\n",
    "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
    "prem = rnn(prem)\n",
    "hypo = rnn(hypo)\n",
    "prem = BatchNormalization()(prem)\n",
    "hypo = BatchNormalization()(hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lovelife/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(600, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/lovelife/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/lovelife/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 42, 300)      12717300    input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 42, 300)      90300       embedding_5[0][0]                \n",
      "                                                                 embedding_5[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 300)          0           time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_5[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 300)          1200        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 300)          1200        lambda_5[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 600)          0           batch_normalization_18[0][0]     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 600)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 600)          360600      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 600)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 600)          2400        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 600)          360600      batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 600)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 600)          2400        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 600)          360600      batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 600)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 600)          2400        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 3)            1803        batch_normalization_22[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 13,900,803\n",
      "Trainable params: 1,178,703\n",
      "Non-trainable params: 12,722,100\n",
      "__________________________________________________________________________________________________\n",
      "Training\n",
      "Train on 549367 samples, validate on 9842 samples\n",
      "Epoch 1/10\n",
      "549367/549367 [==============================] - 208s 379us/step - loss: 0.7674 - acc: 0.6747 - val_loss: 0.6173 - val_acc: 0.7540\n",
      "Epoch 2/10\n",
      "549367/549367 [==============================] - 206s 376us/step - loss: 0.6341 - acc: 0.7439 - val_loss: 0.5618 - val_acc: 0.7810\n",
      "Epoch 3/10\n",
      "549367/549367 [==============================] - 207s 376us/step - loss: 0.6004 - acc: 0.7631 - val_loss: 0.5460 - val_acc: 0.7922\n",
      "Epoch 4/10\n",
      "549367/549367 [==============================] - 207s 377us/step - loss: 0.5824 - acc: 0.7729 - val_loss: 0.5287 - val_acc: 0.8042\n",
      "Epoch 5/10\n",
      "549367/549367 [==============================] - 207s 376us/step - loss: 0.5695 - acc: 0.7807 - val_loss: 0.5236 - val_acc: 0.8032\n",
      "Epoch 6/10\n",
      "549367/549367 [==============================] - 213s 388us/step - loss: 0.5589 - acc: 0.7863 - val_loss: 0.5122 - val_acc: 0.8101\n",
      "Epoch 7/10\n",
      "549367/549367 [==============================] - 227s 413us/step - loss: 0.5515 - acc: 0.7909 - val_loss: 0.5102 - val_acc: 0.8132\n",
      "Epoch 8/10\n",
      "549367/549367 [==============================] - 228s 415us/step - loss: 0.5450 - acc: 0.7952 - val_loss: 0.5152 - val_acc: 0.8150\n",
      "Epoch 9/10\n",
      "549367/549367 [==============================] - 231s 420us/step - loss: 0.5392 - acc: 0.7988 - val_loss: 0.5008 - val_acc: 0.8231\n",
      "Epoch 10/10\n",
      "549367/549367 [==============================] - 234s 426us/step - loss: 0.5339 - acc: 0.8020 - val_loss: 0.5189 - val_acc: 0.8174\n",
      "9824/9824 [==============================] - 1s 150us/step\n",
      "Test loss / test accuracy = 0.5148 / 0.8103\n"
     ]
    }
   ],
   "source": [
    "joint = merge.concatenate([prem, hypo])\n",
    "joint = Dropout(DP)(joint)\n",
    "for i in range(3):\n",
    "    joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, W_regularizer=l2(L2) if L2 else None)(joint)\n",
    "    joint = Dropout(DP)(joint)\n",
    "    joint = BatchNormalization()(joint)\n",
    "\n",
    "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
    "\n",
    "model = Model(input=[premise, hypothesis], output=pred)\n",
    "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training')\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "\n",
    "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\n",
    "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
    "\n",
    "model.load_weights(tmpfn)\n",
    "\n",
    "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
    "\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************\n",
      "6\n",
      "Text:  A woman with a green headscarf , blue shirt and a very big grin .\n",
      "Hypothesis:  The woman has been shot .\n",
      "******************************************************************************************\n",
      "7\n",
      "Text:  An old man with a package poses in front of an advertisement .\n",
      "Hypothesis:  A man poses in front of an ad .\n",
      "******************************************************************************************\n",
      "8\n",
      "Text:  An old man with a package poses in front of an advertisement .\n",
      "Hypothesis:  A man poses in front of an ad for beer .\n",
      "******************************************************************************************\n",
      "9\n",
      "Text:  An old man with a package poses in front of an advertisement .\n",
      "Hypothesis:  A man walks by an ad .\n",
      "******************************************************************************************\n",
      "10\n",
      "Text:  A statue at a museum that no seems to be looking at .\n",
      "Hypothesis:  The statue is offensive and people are mad that it is on display .\n",
      "******************************************************************************************\n",
      "11\n",
      "Text:  A statue at a museum that no seems to be looking at .\n",
      "Hypothesis:  There is a statue that not many people seem to be interested in .\n",
      "******************************************************************************************\n",
      "12\n",
      "Text:  A statue at a museum that no seems to be looking at .\n",
      "Hypothesis:  Tons of people are gathered around the statue .\n",
      "******************************************************************************************\n",
      "13\n",
      "Text:  A land rover is being driven across a river .\n",
      "Hypothesis:  A Land Rover is splashing water as it crosses a river .\n",
      "******************************************************************************************\n",
      "14\n",
      "Text:  A land rover is being driven across a river .\n",
      "Hypothesis:  A vehicle is crossing a river .\n",
      "******************************************************************************************\n",
      "15\n",
      "Text:  A land rover is being driven across a river .\n",
      "Hypothesis:  A sedan is stuck in the middle of a river .\n",
      "******************************************************************************************\n",
      "16\n",
      "Text:  A man playing an electric guitar on stage .\n",
      "Hypothesis:  A man playing banjo on the floor .\n"
     ]
    }
   ],
   "source": [
    "for i in range(6,17):\n",
    "    text=test_org[0][i-1]\n",
    "    hypothese=test_org[1][i-1]\n",
    "    print(\"***\"*30)\n",
    "    print(i)\n",
    "    print(\"Text: \",text)\n",
    "    print(\"Hypothesis: \",hypothese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "Round 1 : please input\n",
      "Text: A woman with a green headscarf , blue shirt and a very big grin .\n",
      "Hypothesis: The woman has been shot .\n",
      "Precicted Judgments:  entailment\n",
      "     \n",
      "Round 2 : please input\n",
      "Text: An old man with a package poses in front of an advertisement .\n",
      "Hypothesis: A man poses in front of an ad .\n",
      "Precicted Judgments:  entailment\n",
      "     \n",
      "Round 3 : please input\n",
      "Text: An old man with a package poses in front of an advertisement .\n",
      "Hypothesis: A man poses in front of an ad for beer .\n",
      "Precicted Judgments:  neutral\n",
      "     \n",
      "Round 4 : please input\n",
      "Text: An old man with a package poses in front of an advertisement .\n",
      "Hypothesis: A man walks by an ad .\n",
      "Precicted Judgments:  contradiction\n",
      "     \n",
      "Round 5 : please input\n",
      "Text: A statue at a museum that no seems to be looking at .\n",
      "Hypothesis: The statue is offensive and people are mad that it is on display .\n",
      "Precicted Judgments:  neutral\n",
      "     \n",
      "Round 6 : please input\n",
      "Text: A statue at a museum that no seems to be looking at .\n",
      "Hypothesis: There is a statue that not many people seem to be interested in .\n",
      "Precicted Judgments:  entailment\n",
      "     \n",
      "Round 7 : please input\n",
      "Text: A statue at a museum that no seems to be looking at .\n",
      "Hypothesis: Tons of people are gathered around the statue .\n",
      "Precicted Judgments:  contradiction\n",
      "     \n",
      "Round 8 : please input\n",
      "Text: A land rover is being driven across a river .\n",
      "Hypothesis: A Land Rover is splashing water as it crosses a river .\n",
      "Precicted Judgments:  neutral\n",
      "     \n",
      "Round 9 : please input\n",
      "Text: A land rover is being driven across a river .\n",
      "Hypothesis: A vehicle is crossing a river .\n",
      "Precicted Judgments:  entailment\n",
      "     \n",
      "Round 10 : please input\n",
      "Text: A land rover is being driven across a river .\n",
      "Hypothesis: A sedan is stuck in the middle of a river .\n",
      "Precicted Judgments:  contradiction\n",
      "     \n",
      "Round 11 : please input\n",
      "Text: \n",
      "Hypothesis: \n",
      "Precicted Judgments:  entailment\n",
      "     \n",
      "Round 12 : please input\n",
      "Text: A man playing an electric guitar on stage .\n",
      "Hypothesis: A man playing banjo on the floor .\n",
      "Precicted Judgments:  contradiction\n",
      "     \n",
      "Round 13 : please input\n",
      "Text: \n",
      "Hypothesis: \n",
      "Precicted Judgments:  entailment\n",
      "     \n",
      "Round 14 : please input\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-25dbe660936a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"     \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Round \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" : please input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mhypothese\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hypothesis: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minferenc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhypothese\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=1\n",
    "while i<=11:\n",
    "    print(\"     \")\n",
    "    print(\"Round \"+str(i)+\" : please input\")\n",
    "    text = input(\"Text: \")\n",
    "    hypothese=input(\"Hypothesis: \")\n",
    "    inferenc=predict_inference(model,text,hypothese)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predicted= model.predict([training[0], training[1]])\n",
    "y_train_predicted=[argmax(x) for x in y_train_predicted]\n",
    "y_train_real=[argmax(x) for x in training[2]]\n",
    "\n",
    "y_valid_predicted= model.predict([validation[0], validation[1]])\n",
    "y_valid_predicted=[argmax(x) for x in y_valid_predicted]\n",
    "y_valid_real=[argmax(x) for x in validation[2]]\n",
    "\n",
    "y_test_predicted= model.predict([test[0], test[1]])\n",
    "y_test_predicted=[argmax(x) for x in y_test_predicted]\n",
    "y_test_real=[argmax(x) for x in test[2]]\n",
    "y_predicted\n",
    "y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_contradiction</th>\n",
       "      <th>pred_neutral</th>\n",
       "      <th>pred_entailment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_contradiction</th>\n",
       "      <td>147628</td>\n",
       "      <td>20826</td>\n",
       "      <td>14733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_neutral</th>\n",
       "      <td>9921</td>\n",
       "      <td>141736</td>\n",
       "      <td>31107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_entailment</th>\n",
       "      <td>3161</td>\n",
       "      <td>12554</td>\n",
       "      <td>167701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pred_contradiction  pred_neutral  pred_entailment\n",
       "                                                                     \n",
       "true_contradiction              147628         20826            14733\n",
       "true_neutral                      9921        141736            31107\n",
       "true_entailment                   3161         12554           167701"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_contradiction</th>\n",
       "      <th>pred_neutral</th>\n",
       "      <th>pred_entailment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_contradiction</th>\n",
       "      <td>2582</td>\n",
       "      <td>387</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_neutral</th>\n",
       "      <td>235</td>\n",
       "      <td>2495</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_entailment</th>\n",
       "      <td>81</td>\n",
       "      <td>224</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pred_contradiction  pred_neutral  pred_entailment\n",
       "                                                                     \n",
       "true_contradiction                2582           387              309\n",
       "true_neutral                       235          2495              505\n",
       "true_entailment                     81           224             3024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_contradiction</th>\n",
       "      <th>pred_neutral</th>\n",
       "      <th>pred_entailment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_contradiction</th>\n",
       "      <td>2524</td>\n",
       "      <td>404</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_neutral</th>\n",
       "      <td>231</td>\n",
       "      <td>2414</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_entailment</th>\n",
       "      <td>89</td>\n",
       "      <td>257</td>\n",
       "      <td>3022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pred_contradiction  pred_neutral  pred_entailment\n",
       "                                                                     \n",
       "true_contradiction                2524           404              309\n",
       "true_neutral                       231          2414              574\n",
       "true_entailment                     89           257             3022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_columns=['pred_contradiction','pred_neutral','pred_entailment']\n",
    "confusion_rows=['true_contradiction','true_neutral','true_entailment']\n",
    "train_confusion_matrix=pd.DataFrame(confusion_matrix(y_train_real, y_train_predicted))\n",
    "train_confusion_matrix.columns=confusion_columns\n",
    "train_confusion_matrix['index']=confusion_rows\n",
    "train_confusion_matrix=train_confusion_matrix.set_index('index')\n",
    "train_confusion_matrix.index.name=''\n",
    "\n",
    "valid_confusion_matrix=pd.DataFrame(confusion_matrix(y_valid_real, y_valid_predicted))\n",
    "valid_confusion_matrix.columns=confusion_columns\n",
    "valid_confusion_matrix['index']=confusion_rows \n",
    "valid_confusion_matrix=valid_confusion_matrix.set_index('index')\n",
    "valid_confusion_matrix.index.name=''\n",
    "\n",
    "test_confusion_matrix=pd.DataFrame(confusion_matrix(y_test_real, y_test_predicted))\n",
    "test_confusion_matrix.columns=confusion_columns\n",
    "test_confusion_matrix['index']=confusion_rows\n",
    "test_confusion_matrix=test_confusion_matrix.set_index('index')\n",
    "test_confusion_matrix.index.name=''\n",
    "\n",
    "display(train_confusion_matrix)\n",
    "display(valid_confusion_matrix)\n",
    "display(test_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
